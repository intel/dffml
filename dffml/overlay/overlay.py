from typing import Any, Dict, NewType, Type, List

from ..base import replace_config
from ..df.system_context.system_context import SystemContext
from ..df.types import Stage, DataFlow, Input, Definition
from ..operation.output import remap
from ..df.memory import MemoryOrchestrator
from ..df.base import op
from ..util.data import merge as _merge
from ..util.entrypoint import base_entry_point, Entrypoint


# TODO Unify this DataFlowType. Used as an example to show auto typing.NewType
# conversion to Definition (Definition is our pre-input linage based mechanism
# for identification of data type).
DataFlowType = NewType("dataflow", object)
# DataFlowTypeAsDict = cast(dict, DataFlowType)
# unittest.mock.patch have cast set val.typ = typ. Parse python source retrieved
# via inspect (stack frame code?) typecheck in the event mock.patch addition of typ not available at runtime.
DataFlowTypeAsDict = NewType("dict.dataflow", object)
UnusedDataFlowTypeAsDict = NewType("unused.dict.dataflow", dict)
DataFlowToApplyAsOverlay = NewType("DataFlowToApplyAsOverlay", DataFlow)
DataFlowWeAreApplyingOverlaysToByRunningOverlayDataflowAndPassingAsAnInput = NewType(
    "DataFlowWeAreApplyingOverlaysToByRunningOverlayDataflowAndPassingAsAnInput",
    DataFlow,
)
DataFlowAfterOverlaysApplied = NewType(
    "DataFlowAfterOverlaysApplied", DataFlow
)

DataFlowBeingOverlayed = NewType("DataFlowBeingOverlayed", DataFlow)
DataFlowBeingOverlayedAsDict = NewType(
    "DataFlowBeingOverlayedAsDict", DataFlow
)
DataFlowAfterOverlaysMerged = NewType("DataFlowAfterOverlaysMerged", DataFlow)


# TODO Example of configurable return type for instance usage within DataFlow
@op
def overlays_installed(
    plugin_base: Entrypoint,
) -> List[DataFlowToApplyAsOverlay]:
    """
    Return a batch of overlays to be applied. The batch is given by searching
    the dffml.overlay entrypiont for installed registered overlays.
    """
    # TODO Have a plugin which itself is used to discover plugin types. We can
    # then reference by string and load the base class for that plugin type.
    # This is really just a stub for that more advanced abitrary version.
    # >>> Plugin.load("dffml.model")
    # Model
    # Iterate over all the installed overlays
    return cls.load()


# For inital overlay load of top level system context, we call
# overlays_installed to get all the dataflows which need to be merged. We then
# execute the merged installed overlays if APPLY_INSTALLED_OVERLAYS
# is given for.
# ``overlay`` on ``run()``. Then we we run, the only default overlay added from
# the main package on install is one which defines an output operation which
# grabs all the dataflows within the input network of the running context for
# the overlay, merges them togther, and returns the to be run dataflow.
DFFML_MAIN_PACKAGE_OVERLAY = DataFlow(
    # Main package overlay will grab any DataFlow objects produced by other
    # installed overlays which consume the DataFlowToApplyAsOverlay Input.
    # It will output a results object with the "overlayed" key set to all of the
    # dataflows generated via other overlays, which consumed the
    # DataFlowToApplyAsOverlay.
    operations={
        "dataflow_todict": op(
            name="dataflow_todict",
            inputs={"dataflow": DataFlowBeingOverlayed},
            outputs={"dataflow_as_dict": DataFlowBeingOverlayedAsDict},
            multi_output=False,
        )(lambda dataflow: dataflow.export()),
        "merge": op(
            name="dataflow_merge",
            stage=Stage.OUTPUT,
            inputs={
                "dst": DataFlowBeingOverlayedAsDict,
                "src": DataFlowToApplyAsOverlay,
            },
            outputs={
                # TODO Full autogenerated definition name should reflect linage
                # of definition. Aka, this thing is a dict
                "merged_dataflow": UnusedDataFlowTypeAsDict,
            },
            multi_output=False,
        )(lambda dst, src: _merge(dst, src.export())),
        "dataflow_fromdict": op(
            name="dataflow_fromdict",
            stage=Stage.OUTPUT,
            inputs={"merged": DataFlowBeingOverlayedAsDict,},
            outputs={"overlays_merged": DataFlowAfterOverlaysMerged,},
            multi_output=False,
        )(lambda merged: DataFlow._fromdict(**merged)),
        "apply_overlay_to_dataflow_to_be_executed": op(
            name="dataflow_fromdict",
            stage=Stage.OUTPUT,
            inputs={
                "merged": DataFlowAfterOverlaysMerged,
                "dataflow_we_are_applying_overlays_to_by_running_overlay_dataflow_and_passing_as_an_input": DataFlowWeAreApplyingOverlaysToByRunningOverlayDataflowAndPassingAsAnInput,
            },
            outputs={"overlayed": DataFlowAfterOverlaysApplied,},
            multi_output=False,
        )(lambda merged: DataFlow._fromdict(**merged).update(auto_flow=True)),
    },
    seed=[
        Input(
            # Adding a blank flow ensures if there are no other
            # overlays, we just end up merging the input dataflow
            # with a blank overlay. Resulting in the return of the
            # original dataflow.
            value=DataFlow(),
            definition=DataFlowToApplyAsOverlay,
        ),
    ],
)


# TODO(alice) Figure out if there is something we need to do with
# regards to the order in which overlays are applied. Can we use their
# input allowlist to detect interdependencies? Do we need to?
# For example:
# >>> Input(value=dffml.DataFlow(), definition=DataFlowBeingOverlayed)
# Maybe relates to DataFlow as class arguments as if they were allowlist
# Input objects for this call are equivalent to the arguments of a
# dataflow as class method. A method is simply an operation within the
# dataflow, which serves as a place to add other inputs perhaps within
# within their own contexts even, but by default same context as the
# operation for the method was executed in.
# Method call:
# Create new context as input object, context maintains link to parent
# context.
# TODO Document if you want to define the order in which overlays are
# applied. You could overlay the default overlay to somehow insure
# iteration over plugins feed to merge is gated. Perhaps take the output
# of merge as an input to a non expanded overlays_installed() operation.
# TODOO For DataFlow as class method, take the flow from DataFlow
# allowlist. To create the DataFlow as class, run a dataflow on another
# dataflow, similar to how we do overlays, to produce the class object.
# Method signatures correspond operations within each flow. Each
# operation becomes a method. There might be multiple methods running at
# the same time interacting with each other within the dataflow as class
# double context entry.
#
# NOTE Below line with DataFlow._method was just for DataFlow as
# as class method calling concepts.
# **DataFlow._method(overlay_batch_apply, DataFlow(overlays_installed), cls.load()),
#
# We require via manifest/did method style schema for output
# probably, it should have an overlayed top level key of data schema
# type matching system context within that an open architecutre
# within that with a dataflow within that.
_DFFML_OVERLAYS_INSTALLED = DataFlow(
    overlays_installed,
    operations={
        "get_overlay_dataflow_after_merged": op(
            name="get_overlay_dataflow_after_merged",
            stage=Stage.OUTPUT,
            inputs={"merged": DataFlowAfterOverlaysMerged,},
            outputs={"merged": DataFlowAfterOverlaysMerged,},
            multi_output=False,
        )(lambda merged_dataflow: merged_dataflow),
    },
)
DFFML_OVERLAYS_INSTALLED = DataFlow._fromdict(
    **_merge(
        DFFML_MAIN_PACKAGE_OVERLAY.export(),
        _DFFML_OVERLAYS_INSTALLED.export(),
    )
)
# We replaced apply_overlay_to_dataflow_to_be_executed with
# get_overlay_dataflow_after_merged
del DFFML_OVERLAYS_INSTALLED.operations[
    "apply_overlay_to_dataflow_to_be_executed"
]
DFFML_OVERLAYS_INSTALLED.update(auto_flow=True)

# Create Class for calling operations within the System Context as methods
DFFMLOverlaysInstalled = SystemContext.subclass(
    "DFFMLOverlaysInstalled",
    {
        "upstream": {"default_factory": lambda: DFFML_OVERLAYS_INSTALLED},
        # TODO(alice) We'll need to make sure we have code to instantiate and
        # instance of a class if only a class is given an not an instance.
        "overlay": {"default_factory": lambda: None},
        "orchestrator": {"default_factory": lambda: MemoryOrchestrator()},
    },
)

# Callee
async def apply_overlays_dffml_installed_overlays(
    dataflow: DataFlow, _dffml_system_context
):
    pass


# Caller
"""
dataflow = await apply_overlays_dffml_installed_overlays(dataflow)

# TODO This first flow execution to create the main package overlay
# with overlays_installed as an operation which is the source of the
# dataflows to apply as overlays. DFFML_OVERLAYS_INSTALLED.
async for ctx, results in orchestrator.run(
    DataFlow(overlay_batch_apply),
    [
        # use_this_dataflow: DataFlowType = DFFML_MAIN_PACKAGE_OVERLAY,
        Input(value=DFFML_MAIN_PACKAGE_OVERLAY, definition=DataFlowType),
        # to_merge_these_overlays: List[DataFlowToApplyAsOverlay],
        Input(
            value=[
                DataFlow(
                )
            ],
            definition=List[DataFlowBeingOverlayed],
        ),
        # into_this_dataflow: DataFlowBeingOverlayed,
        Input(value=DFFML_MAIN_PACKAGE_OVERLAY, definition=DataFlowBeingOverlayed),
        # and_to_apply_merged_overlay_to_this_dataflow: DataFlowWeAreApplyingOverlaysToByRunningOverlayDataflowAndPassingAsAnInput,
        Input(value=DataFlow(), definition=DataFlowWeAreApplyingOverlaysToByRunningOverlayDataflowAndPassingAsAnInput),
    ],
):
    pass


async def main():
    async with SystemContext(
        inputs=[
        ],
        architecture=DataFlow(
        ),
        orchestrator=MemoryOrchestrator(),
    ) as sysctx:
        await sysctx.overlay_batch_apply(
            # to_merge_these_overlays,
            # into_this_dataflow,
            # and_to_apply_merged_overlay_to_this_dataflow,
            # use_this_dataflow=use_this_dataflow,
            use_this_dataflow=use_this_dataflow,
        )

"""


@op(stage=Stage.OUTPUT,)
async def overlay_batch_apply(
    self,
    to_merge_these_overlays: List[DataFlowToApplyAsOverlay],
    into_this_dataflow: DataFlowBeingOverlayed,
    and_to_apply_merged_overlay_to_this_dataflow: DataFlowWeAreApplyingOverlaysToByRunningOverlayDataflowAndPassingAsAnInput,
    *,
    use_this_dataflow: DataFlowType = DFFML_MAIN_PACKAGE_OVERLAY,
) -> dict:
    # TODO(alice) Figure out if there is something we need to do with
    # regards to the order in which overlays are applied. Can we use their
    # input allowlist to detect interdependencies? Do we need to?
    # For example:
    # >>> Input(value=dffml.DataFlow(), definition=DataFlowBeingOverlayed)
    # Maybe relates to DataFlow as class arguments as if they were allowlist
    # Input objects for this call are equivalent to the arguments of a
    # dataflow as class method. A method is simply an operation within the
    # dataflow, which serves as a place to add other inputs perhaps within
    # within their own contexts even, but by default same context as the
    # operation for the method was executed in.
    # Method call:
    # Create new context as input object, context maintains link to parent
    # context.
    async with self.subflow(use_this_dataflow) as octx:
        async for ctx, results in octx.run(
            [
                Input(value=dataflow, definition=DataFlowToApplyAsOverlay)
                for dataflow in to_merge_these_overlays
            ]
            + [
                Input(
                    value=into_this_dataflow, definition=DataFlowBeingOverlayed
                )
            ]
            + [
                Input(
                    value=and_to_apply_merged_overlay_to_this_dataflow,
                    definition=DataFlowWeAreApplyingOverlaysToByRunningOverlayDataflowAndPassingAsAnInput,
                )
            ],
        ):
            pass
        # We require via manifest/did method style schema for output
        # probably, it should have an overlayed top level key of data schema
        # type matching system context within that an open architecutre
        # within that with a dataflow within that.
        return results["overlayed"]


@base_entry_point("dffml.overlay", "overlay")
class Overlay(DataFlow, Entrypoint):
    @classmethod
    async def old_default(cls, orchestrator):
        # TODO(alice) Figure out if there is something we need to do with
        # regards to the order in which overlays are applied. Can we use their
        # input allowlist to detect interdependencies? Do we need to?
        # For example:
        # >>> Input(value=dffml.DataFlow(), definition=DataFlowBeingOverlayed)
        # Maybe relates to DataFlow as class arguments as if they were allowlist
        # Input objects for this call are equivalent to the arguments of a
        # dataflow as class method. A method is simply an operation within the
        # dataflow, which serves as a place to add other inputs perhaps within
        # within their own contexts even, but by default same context as the
        # operation for the method was executed in.
        # Method call:
        # Create new context as input object, context maintains link to parent
        # context.
        async for ctx, results in orchestrator.run(
            DFFML_MAIN_PACKAGE_OVERLAY,
            [Input(value=DataFlow(), definition=DataFlowBeingOverlayed)],
            orchestrator=orchestrator,
        ):
            pass
        # We require via manifest/did method style schema for output
        # probably, it should have an overlayed top level key of data schema
        # type matching system context within that an open architecutre
        # within that with a dataflow within that.
        return results["overlayed"]

    async def apply(self, orchestrator, dataflow):
        # TODO this should become an operation and then used as dataflow as
        # class style
        # TODO(security) Some method to audit if org overlays were taken into
        # account  within explicitly passed overlay
        async for ctx, results in orchestrator.run(
            self,
            [Input(value=dataflow, definition=DataFlowToApplyAsOverlay)],
            orchestrator=orchestrator,
        ):
            pass
        # We require via manifest/did method style schema for output
        # probably, it should have an overlayed top level key of data schema
        # type matching system context within that an open architecutre
        # within that with a dataflow within that.
        return results["overlayed"]
