# [Architecting Alice: Volume 0: Preface](https://github.com/intel/dffml/discussions/1369#discussion-4023096)

> Preface to tutorial series - May 2022 - June 2023 (planned end date, last post for volume 1 ends there.)

## Engineering Logs

- [Volume 0: Context](https://youtube.com/playlist?list=PLtzAOVTpO2jaHsS4o-sDzDyHEug-1KRbK)

### Table Of Contents

### Volume 0

- [Volume 0: Preface](https://github.com/intel/dffml/discussions/1369#discussion-4023096)
- [Volume 0: Introduction and Context]()

### Volume 1

- [Volume 1: ]()

## Notes

Much of this discussions thread are notes and scratch work around the purpose and future of the project. Everything here will be converted to ADRs, issues, code, etc. as appropriate. We as a community (open to everyone) will work together to map our our activites to achive these goals. We will document our process along the way and write these series of tutorials to show others how they can understand and extend the open architecture (Alice).

Jump to this https://github.com/intel/dffml/discussions/1369#discussioncomment-2603280 comment for more details on Alice the maintainer blog post series.

DFFML's Mission: Provide a clear, meticulously validated, ubiquitously adopted reference architecture for a freedom and privacy preserving egalitarian Artificial General Intelligence (AGI) which respects the first law of robotics.

To do so we must enable the AGI with the ability to act in response to the current system context where it understands how to predict possible future system contexts and understands which future system contexts it wishes to pursue are acceptable according to guiding strategic plans (such as do no harm). We must also ensure that human and machine can interact via a shared language, the universal blueprint.

AI has the potential to do many great things. However, it also has the potential to to terrible things too. Recently there was an example of scientists who used a model that was good a generating life saving drugs, in reverse, to generate deadly poisons. GPU manufacturers recently implemented anti-crypto mining features. Since the ubiquitous unit of parallel compute is a GPU, this stops people from buying up GPUs for what we as a community at large have deemed undesirable behavior (hogging all the GPUs). There is nothing stopping those people from buying for building their own ASICs to mine crypto. However, the market for that is a subset of the larger GPU market. Cost per unit goes up, multi-use capabilities go down. GPU manufacturers are effectively able to ensure that the greater good is looked after because GPUs are the ubiquitous facilitator of parallel compute. If we prove out an architecture for an AGI that is robust, easy to adopt, and integrates with the existing open source ecosystem, we can bake in this looking after the greater good.

As we democratize AI, we must be careful not to democratize AI that will do harm. We must think secure by default in terms of architecture which has facilities for guard rails, baking safety into AI.

Failure to achieve ubiquitous adoption of an open architecture with meticulously audited safety controls would be bad. The best defense is a good offense.

> Most of what's on this thread is just putting enough words to make us remember the related stuff later and fill it out so it makes sense. This thread is a central place for everyone interested to participate and collaborate. There are many pieces to this plan that need to be driven by many individuals to make this all happen. Reach out or just start commenting if you want to get involved.