Initial thoughts formed by strategic plans kickstarted at beginning of context kick off. They begin spinning off thoughts (system contexts) which could be moved from start k8s start to template spec state. These are passed to the gatekeeper and then to the prioritizer. Other strategic plans can accept outputs of strategic plans and then generate better guesses at system context. We are guessing at the chemical equation required to satisfy the seed input (the voice command). The prioritizer is trying all ideas it can in order to satisfy the request. Some it really executes, some it just thinks about and does accuracy calculations across nested startigc plan predictions given best guess seed inputs using available resources through operations to achive outputs that satisfy agent, organizational, and requestor statigic plans (withing yhe top level systems context). You have thoughts and hypothesis and executions all running until some end condition. Can be resummoned later by asking how something is going. Alice can go check on active or cached trains of thought related to a query about a previous seed. She can report state of trains of thought. If objectives were completed and what dataflows and system contexts helped us meet those objectives. What chains of system contexts lead to each other and what events effected each other. We look at system contexts over time like frames in a video for time based context within a tree of a train of thought. We then need to look across multiple trains of thought to identify communalities. This is like looking across multiple video feeds from different angles to reconstruct 3d models. Its a matrix to matrix mapping of I/O matrix to startigic plan matrix. Given these inputs make me a multioutput model for the statigic inputs, stratigic outputs, and outputs. Do this for all combinations for all statigic plans. Ensure structured loged data such as time to complete operation are captured.

- Dataflow as class.
- Intercept Inputs with caching (save/load but make it a generic dataflow so we can save in background thread while we retrain all the models on the new input. that new input created an entirely new system context within that train of thought. And we have added its (DNA) encoded version to the memory of the strategic plans (input or output variations produced by combination training).
  - Train as many models as possible on intercepted data.
    - Models are used within stratigic plans to make predictions that are accuracy tested on historical data. Idealy we can move to a system where we have known trustworthy historical data and attempt to reduce the training dataset to a small enough set to still achive accuracy that staisfies overall goals.
    - accuracy of models should be feed into a model with other inputs being strategic plan input output and inputs and outputs for system context we can make a model to predict the classification from the gatekeeper. Gatekeeper ensures dont hurt people.
    - multioutput or encoder/decoders should be trained on different permutations of the inputs and outputs.
    - We should attempt to prioritize the training of models by having strategic plans that produce metrics mapping to choices of training.
      - strategic plan scores low for model architecture, using old one detected, output metric. We looked at devs code and they are making less than informed choice. Ensure this knowledge makes it into prioritizer for  scheduling of this dataflow/system context.
      - we can use this to build a model to predict which thoughts we think the prioritizer will like. Train model to map system contexts (preprocess to DNA for operations, probably encoding tables for operation names and definition names and primitives) and inputs and outputs to prioritization. This model and this set of model permutations will be available to strategic plans to 
    
Stratigic plans are usually just output operations

We can say train and give the strategic plan model one record which is the system context which is the same memory record passed to the caching functionality. that system context could of course go through preprocessing flow before being used for training. This preprocessing flow might be where we put our system context DNA encoder.
    

Could also try language model on flattened I/O? I think not as good. But try later.

on k8s crd creation we effectively have an Alice start which is given system context. These inputs are feed to stratigic plans as described above. The prioritizer decides what system contexts it wants to try executing for real and what ones it just wants to think about. By default in effect we run in safe mode. No execution. No reaction. Just hypothesize. All the strategic plans in those system context thoughts? Call their predict method. When you do that. Youâ€™ll be relying on models trained from input output values of saved cached system contexts.  Files saved. 

We need to be rerunning the accuracy on the strategic plan model after every execution. This will have the effect that in Flash Boys where everyone wanted to recreate the SEC approved something number so they could abitrare faster than the latency of the time it took others without the model to see the new price. We want to predict dataflows that satisfy the output constraints. We do this by using our stratigic plan to I/O models.

if everything is a encoder then create encoder/decoder models on all permutations of inputs and outputs across stratigic plans and system context I/O

prediction from models can become values for stub operations

We can leverage task 2: caching to use sources to map input values within system contexts including strategic plan inputs and outputs into dataset sources. We can leverage dataflows to modify data in different ways as it is saved to and loaded from the cache. For example run inputs with locality of file through uploads to blob storage or personal datastore. Then create references in db A and update references used in 

play with DIDs and personal datastores before making any input networks. Create proxies from web2 to web3 which are DFFML agnostic but are packaged as dffml-service-web3-did|datastore|relay|bdrige-irc|smtp etc.

For caching/input interception record to asciinema streams. Have an asciinema source that supports when a record is saved use its features as timestamps and save stream or send out next line to listeners via pub/sub or otherwise to facilitate watching trains of thought execute live.