value of data is defined as the datas relative weight when determining importance by a prioritizer. Different data has different value within different contexts as defined by each contexts prioritizer. Rewards for execution based on alignment with reward givers prioritizer output for a system context (agent, user, org). This means we can incentivize verbatim execution of trains of thought (attestation, etc.) Or we can incentivize agents who we see are doing activities which have strong correlation between our priorities. We see this using outputs of strategic plan models. We train and encoder/decoder on the prioritizer I/O across models. This will be like and encoder/decoder for images of different sizes. Maybe its a GAN? Where we use the GAN in either direction to encide decode to new reality. Then we have another model which takes predictions from GAN and trys to increase their accuracy by mapping them to the prioritizer I/O in the same dimension (image X/Y size). This is in effect our alignment model. It has its own strategic plan. If the accuracy is high we have high alignment. We should somehow encourage the entities producing the outputs from the system contexts under analysis to continue doing good work that is work weâ€™d otherwise have to be doing.