- to generate new system contexts
  - if the data type looks like a match on an operation input use the trained adaptive sandboxing models to predict if the input falls in a non error cluster. Error is defined by exceptions raised during operation execution. Perhaps we should have an EXCEPTION Stage. Allow registration on exception handlers via operations with inputs whose datatypes match exceptions raised. This stems from our typing unification. We should extend further to have types optionally defined potentially even ad hoc (on overlay) so as to signify the path they should have taken. The past N hops in operations. ensure went through input validation matching interface X. Ensure valid / known type transformation using models described above to predict when a new liniage is defined / proposed the hypothesis on its feasibility. Feadsbility will be an output generated by a starategoc olan whcoh is running the sandboxing model from the anomoly detection  strategic plan. If it lands on an error cluster prediction feasibility will say this context may be a valid context based on type transformations of underlying primitives, but since your asking me (the feasibility strategic plan) i think its a no go. Executing this thing probably wont work. You can then prioritize that lower in terms of trains of thought thinking in that direction. We should always be running strategic plans 
- an algorithm for a generic entity, modeling the mind
- Tune scaling methods via locality and trust boundary topologies 
- Always think in parallel. Use response time to know length if time we can explore train of thought before we need to send control signals associated with that train of thought.
- You can look at any problem (i.e. system context) through N different lenses. Through the eyes of N different entities. What each person “sees” or rather, experiences, is a combination of raw data (feature extraction) and feel. Mind. interpretation. Perspective. The results from different output flows overlayed on a cached or in progress execution, hypthesis, etc. 
- https://github.com/cilium/tetragon
- https://github.com/tokio-rs/axum
- https://mdformat.readthedocs.io/en/stable/users/installation_and_usage.html#python-api-usage
- https://twitter.com/bernhardsson/status/1526635195243409408
  - > Something that old boring process literature talk about (Toyota, The Goal etc) that applies to software engineering 10000000% is that minimizing the size of the feedback loop is much more important for productivity than minimizing idleness
  - Why? The more useful data we have, the more precise we can be with the control portion of our feedback loop. Splitting into small feedback loops (like refactoring into a set of functions with locality or trustbonudries understood) helps us measure more granularly.
- TODO
  - `alice new architecture <project-name>`
    - Have alice guess based on parent system context and project name what kind of project she should create (operations -> within `operations/` or name `-operations-<something>`
    - Run setup tasks overlayed from parent system context
      - You use github? Create a github repo for it
      - You want to register a project name with your container registery you usally use, right?