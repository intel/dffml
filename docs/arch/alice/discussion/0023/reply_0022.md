- https://youtu.be/A-S9Z684o4Y
- The Open Architecture enables hybrid on/off chain smart contacts.
  - It does this by incorporating risk management into architecture definition / smart contract.
    - Strategic plans work with the gatekeeper and prioritizer to negotiate and execute off chain contracts.
    - Models within strategic plans are involved in risk tolerance.
  - Smart contracts with understanding of risk are effectively entities making decisions based on models. This allows for mutation in implementation while maintaining principles.
    - The smart contract is able to make its own decisions based on learned experience (models) so as to continue to operate until its strategic goals are meet. As measured by oracle data ordained from trusted parties as is applicable to context. Where chains of trust are established via Peer DIDs between entities and data for provenance. Leveraging verifiable credentials (opencert) for review system to measure risk in absence of attestation.
- For trains of thought at critical velocity. Fully optimized resource utilization to optimally progress train of thought to move strategic principles in correct directions and advance the “state of the art” for any given conceptual field. A/B feature branch permutation testing to sus out bleeding edge to determine system context which is the optimal selection of overlays where overlays are forks, branches, similar repos, similar projects, similar specs, similar working groups, similar goals, similar strategic plans, similar strategic principles, priorities. Autoencoders trained against input data for system context with high accuracy represent.
- This set of operations used maps to these strategic plan output through conceptual layers. Could use classification model to buckeize into key value map then lookup image, ir other data and output that. So this is like saying i have a cached system context where i want to visualize the codebase as if it was a cartoon character. We then classify manually or via strategic plans which suggest classifications. Which codebases are similar to which characters. We then run feature extraction operations / strategic plans which suggest system contexts to do feature extraction based on definition aka input parent linage / locality / primitive. This means what data flows can we make where the data on either side of the manual classification is taken as inputs. Do this for each side. Filter down to valid flows as defined by possible routes of inputs within parent system context to inputs of operations in suggested flows. This is similar to making a balanced chemical equation. Again using out analogy of the chemical equation. We build a encoder/decoder models of all permutations of strategic to strategic and other inputs (each unique liniage as a record). So you could have an input which says which codebase is this similar too. Good, bad, unsure. Then a label for cartoon characters with good bad unsure. Then run feature extraction on each. After auto encoder modules are built
- Universal translator with understanding of meaning. Map one representation to another by thinking up as many system contexts as possible which describe each representation. All possible features we can extract. All possible dataflows we can build by wiring together different compatible types by creating all permutations of all interfaces including nesting. During building of complete set filter to valid system contexts checking possible routings of inputs within parent system context or ability to create from those.
- Distributed network of metric collectors. Of security information we should feed into cve bin tool. Maybe start by creating checker entrypoint for a checker which knows it’s running under a dataflow. Could execute using runpy with opimp self in globals. OpImp shared config property of object which is dataflow as class which listens for new vuln info for checker in background when instantiated. When a new vuln is detected we could trigger a scan on all previously scanned atrifacts for which we had scanned before by having a strategic plan overlayed on a long running flow which inspects historical contexts which executed scans against checker came up as exists within file scanned. Use this presence of existence within previous scans to query off chain data from historical system contexts. To build next contexts where results of scan opperatuon are removed so that running results in latest vuln info being incorporated into scan. This is analogous to dev pull model when new commit on branch released. Do scan rerun is same as redoing A/B feature testing of commits.
- Prioritizer opportunity cost factor in value of data extracted from running self vs cost to contract and loss of data value. Data value measured by historical instances where output of the model was found to have strong correlation with positive changes in strategic principles. Then map those principles via conceptual translation model to measure(s) of value to compare apples to apples in terms of cost to execute self vs contract.
- Learning on the job. Pair no/low experience agents with agents they can learn the most from when forming ad-hoc organizations (teams) between agents to work towards overall goals across a set of subsystems or projects / repos / trains of thought.
- Commits in repo map to train of thought. Commit is system context. Train of thought is branch.
- Overloaded
  - Hypothesis: Optimal performance exists in the via a diverse set of trains of thoughts which can only be achieved when agents are involved in more initiatives than they can feasibly make progress on. Agents will therefore either strategically "drop packets" (trains of thought that were progressing but all the sudden do not progress, didn't work on any issues within a given project this week for example) or shoot for a lost train of thought metric (number of dropped packets / time) which is come up with via analysis of equilibrium model for current state so as to account for "acceptable losses" within a train of thought when optimizing for overall strategic principles. This is like over booking. Could use strategic plans which output metrics on system contexts to feed into prioritizer. Prioritizer could have some sort of dont schedule or prioritize until we have this core set of metrics accounted for (hypothesized or tested). Prioritizer will also pull from other maybe global or train of through specific metrics to decide opportunity cost in relation to moving strategic principals. Via requesting runs or waiting for in progress runs of contexts which could have handles maybe allowing us to track their parents. Maybe their handles are an input object where the input object can be received asynchronously via prioritizers input network. This approch works on individual trains of thought up through arbitraryly complex organizational groups working together because the tree like model feeding into the top level strategic principles is the same. It is this pyramid which is the construct of prioritization. What we enable via cross domain conteptual mapping is reconstruction of prioritization pyrimids wothin the alternate conceptual domain. Within the alternative strategic principles. The pyrimid like nature of our conceptual levels allows us to to think of those influenceing factors on outcomes of higher conceptual level models (maping to feature extraction as well if going to the iceburg under the pyrimid). We can think of the influences like the weight distribution of block on the top onto those below down to the base. The feature (all inputs within collector flow) to straetigc plan output models with high accuracy is like finding out whoch peices of the top pyrimid have “energetic” connections to spots in theower half of the iceburg below. When we build ad host organizations these themseves act like the entities wothin them and all the data therefore like the suffleing around of the blocks within their pyrimids to create a new pryimid with some additional blocks in it which are what make it facilitate collaboration and communication within rhe group. So prioritizes and stratigies for achive those prioritys using agents / assests within the group.
- plan
  - on chain, herstory of all thoughts (LOL it really is flip mode would ya look at that!)
    - ensure that we can import export Input objects to Peer DID chain with Input instances as NFTs. Later maybe with allowlist of encoding of values which strategic plans can be overlayed to think up system contexts that convert data types in inputs to off chain storage.
  - Dataflow as class
  - Operation for dataflow as class methods
  - instantiate operations from top level system context from auto start background contexts. Like our current implementation of datflow.seed but for contexts.
  - Implement caching 
- What will Alice do?
  - Architect
  - DJ
  