How we can successfully foster innovation? Reward successful trains of thoughts with more effort to see how they play out and what new system contexts they generate. Be careful not to do too much work without seeing ROI. Don't keep working on a job if you aren't getting paid. Estimate likelihood of getting paid based off frequency. Measure time and energy (compute cycles) put in and correlate with pay to decide what to work on based on ROI for Alice.

There is an equilibrium between chaos and complete control (measured as 100% of inputs produced within a system context, including all subflows/contexts, are consumed by strategic plans, meaning we are taking every possible thing into account before issuing new system contexts to be executed) where optimal performance is measured as the number of system contexts being executed successfully.

Usage stats of a universal blueprint within downstream blueprints should be taken into account by a strategic plan which vets new thoughts (dataflows/+system contexts) to prioritize (HAVEN'T LOOKED AT THIS YET, flush this out) thoughts which are executing within successful trains of thought relative to pace of progress of other trains of thought (clustering model on dataflows/system contexts to determine similar trains of thought).

After new system contexts are issued by strategic decision maker, there should be a prioritizer which decides which thoughts get played out (dataflows with system context executed) on what available resources (orchestrators).

Streamline the research to usage pipeline of the ML ecosystem (researchers making models and software engineers using them in real world applications). Make taking from ideation phase to production trivial, including deployment to any environment (edge). Effectively create a unified programming interface across UI/client and server. Combining threat model data with description of program flow allows us to have dynamic control over deployment to satisfy confidentiality, integrity, and availability (CIA) goals. Leverage this architecture to enable analysis of arbitrary code bases (meta static analysis). Finally, execute the scientific process to come up with alternate program flows/architectures which satisfy strategic goals beyond maintenance of CIA assurances (changes to overall purpose of program, optimize for cost over speed, etc.). This work centers around data flow based descriptions of architectures as they provide observability, an easily machine modifiable structure, and act as a vehicle for communication of intent around asset handling.

Build an AI that can program and actively tests out it's programs. Data flow approach is a great way to get there due to the properties on observability it provides which allow us to train models on everything it does to optimize it for specific use cases as well as discover what other possibilities for program flows their could be.

DataFlows allow us to compare apples to apples for code written in different languages.

The universal blueprint is a proxy for domain specific descriptions of architecture.

Operations should expose (historical) data on timeouts clients (when remote) should try waiting before raising timeout issues.

It's a little all over the map, just trying to solve the problem that most things are an integration problem. And maybe build some kind of AI along the way. we're just writing the same code over and over in different variations and it's time the computer just did it for us.

We want to be able to turn insights from domain experts into realized ROI as fast as possible. We want to reward these useful thoughts.