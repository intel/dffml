Hi Hammond,

I saw your talk to ____ and wanted to connect with you. Our areas of research appear to be aligned based on [your blog’s research agenda page](https://www.cyberhammond.com/research-agenda).

We’ve been thinking about what underlying infrastructure (Decentralized Identifiers, Verifiable Credentials, etc.) needs to be in place to enable a holistic approach to software maintenance, generation, and ongoing security. We’re still in the early stages. Alignment of AI generated code to strategic principles, plans, and values (such as security standards) is shaping up to be an area of interest.

We’ve been planning and starting the implementations of Alice, a reference entity which is her own threat model, described via an Open Architecture and Living Threat Model: https://github.com/intel/dffml/tree/alice/docs/tutorials/rolling_alice/0000_architecting_alice#what-is-alice
The goals are to triangulate the “soul” of the software via static and behavioral analysis and mapping that to intent via trust boundaries defined via the architecture and threat model.

I was curious about your work and how you approach or plan approaching alignment of generated code to intent. Do you have any methods which look promising for capturing intent? Threat modeling only covers security, there may be other places it helps with intent. However, I’m sure there are other methods which would be good to explore.

Thank you,
John
